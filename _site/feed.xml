<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-06-20T12:58:33+01:00</updated><id>http://localhost:4000/feed.xml</id><entry><title type="html">Understanding MedCAT</title><link href="http://localhost:4000/blog/2023/06/20/medcat/" rel="alternate" type="text/html" title="Understanding MedCAT" /><published>2023-06-20T00:00:00+01:00</published><updated>2023-06-20T00:00:00+01:00</updated><id>http://localhost:4000/blog/2023/06/20/medcat</id><content type="html" xml:base="http://localhost:4000/blog/2023/06/20/medcat/"><![CDATA[<p>MedCAT <a class="citation" href="#Kraljevic2021-ln">(Kraljevic et al., 2021)</a> is a popular tool for recognising clinical terms in text, and linking to the respective concept from an ontology like SNOMED-CT, ICD-10 or UMLS. It also has the facility to automatically classify concepts based on their context in text.</p>

<p>I’ve been using MedCAT at work, so have become rather familiar with it, but if you spot any errors, DM me on twitter.</p>

<p>MedCAT is available on GitHub: <a href="https://github.com/CogStack/MedCAT">github.com/CogStack/MedCAT</a>, and the MedCAT paper here: <a href="https://arxiv.org/abs/2010.01165">Multi-domain Clinical Natural Language Processing with MedCAT: the Medical Concept Annotation Toolkit</a>.</p>

<p>Official tutorials are <a href="https://github.com/CogStack/MedCATtutorials">here</a>.</p>

<p>Being made of a few quite different parts, MedCAT can be a bit tough to understand well, so here’s an explaination of some bits that I found tricky.</p>

<h2 id="summary">Summary</h2>
<p>MedCAT passes over input text to find clinical concepts. It does this in a few steps:</p>
<ol>
  <li>Spell-checking and correction.</li>
  <li>Tokenisation and lemmatisation.</li>
  <li>Entity recognition: it looks for occurances of terms from its database (“Concept Database” – CDB). When it finds a term, it maps it to the concepts it could be referring to.</li>
  <li>Linking: If there are multiple concepts the term could be referring to, MedCAT looks at the context of the term to determine which concept it represents.</li>
  <li>Tagging: If you have added any “metacat models” to your configuration, then those models are used to add tags based on the context of the detected concepts.</li>
</ol>

<h2 id="how-medcat-does-it">How MedCAT Does It</h2>
<p>Before going over the steps in detail, we should establish what a MedCAT model is. It has two main components: a <em>Vocab</em> and a <em>Concept Database</em>.</p>

<p>The Vocab is a large set of words and their word embeddings. We’ll assume for now that it contains most words in the language. It’s essentially a table:</p>

<table>
  <thead>
    <tr>
      <th>WORD</th>
      <th>COUNT</th>
      <th>VECTOR</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>asprin</td>
      <td>123213</td>
      <td>0.232141 0.141412 …</td>
    </tr>
    <tr>
      <td>hospital</td>
      <td>34244</td>
      <td>0.254252 0.656568 …</td>
    </tr>
    <tr>
      <td>aeroplane</td>
      <td>241415</td>
      <td>0.54555 0.6366 …</td>
    </tr>
    <tr>
      <td>field</td>
      <td>344344</td>
      <td>0.3555 0.878876 …</td>
    </tr>
    <tr>
      <td>teapot</td>
      <td>43452236</td>
      <td>0.52253 0.23333 …</td>
    </tr>
  </tbody>
</table>

<p>The Concept Database (CDB) contains the set of terms of interest, concept codes, <em>Context Embeddings</em>, and mappings between them.</p>

<p>Context embeddings are representations of the words which usually occur around a concept, made by averaging the vectors of the <em>n</em> words either side of the concept. They are used to tell which concept an ambiguous term is referring to.</p>

<h3 id="the-steps">The Steps</h3>
<h4 id="spelling">Spelling</h4>
<p>MedCAT fixes spelling for terms that appear in its database so that it doesn’t miss concepts due to spelling errors. It uses Peter Norvig’s spell checker. You don’t really need to worry about how this bit works.</p>

<h4 id="tokenisation-and-lemmatisation">Tokenisation and Lemmatisation</h4>
<p>This uses a model from SpaCy for this bit - this is why you need to download <code class="highlighter-rouge">en_core_web_md-3.4.0</code> to run MedCAT. Again, this isn’t really a part you’ll have to worry about.</p>

<h4 id="entity-recgonition">Entity Recgonition</h4>
<p>This is where the important bit happens. MedCAT passes over the tokenised text and uses a simple lookup against its list of known concept terms (the CDB), and if there’s a match, it marks that term. It can handle some variation in order, and stop words aren’t an issue as they were removed in the previous step; more detail in the paper.</p>

<p>Then, there’s another pass over all the marked terms. For each term, MedCAT looks it up in the CDB and if it it’s mapped to <em>one</em> concept it links to that concept. If there’s more than one concept that could match, then is has to do some disambiguation.</p>

<h5 id="disambiguation">Disambiguation</h5>
<p>This is were the contect embeddings come in handy. MedCAT computes the context embedding for the term as found in the text. It then compares this to context embeddings that it’s previously learnt for each concept. It scores the similarity with each candidate concept, filters out any that fall below a threshold, and selects the closest one.</p>

<p>It actually computes this similarity score for all concepts, whether or not disambiguation was needed, and returns is to be used as a confidence metric.</p>

<h4 id="tagging">Tagging</h4>
<p>MedCAT provides the option to add <em>metacat</em> “meta-models” to the end of the process which use Bi-LSTM models to add metadata to your detected concepts by tagging them.</p>

<p>These models look at the context of the concepts in text. They are configured by the user and need to be trained manually.</p>

<p>You can use these to do things like recognise when a concept is negated, or to find the laterality of a concept.</p>

<h2 id="internals-and-training">Internals and Training</h2>
<p>If they don’t already, things should start to make more sense as we see how they’re built.</p>

<p>By and large, most of the details for building and training a model are here: <a href="https://github.com/CogStack/MedCATtutorials">MedCAT Tutorials</a>.</p>

<h3 id="vocab">Vocab</h3>
<p>The vocab is used as a set of correctly spelt words for spell checking, and as a corpus for training word embeddings; MedCAT uses Word2Vec by default.</p>

<p>You can make your own word embeddings any way you like, as long as you put them in a file of the right format you can load them into a vocab – see the tutorials for this.</p>

<p>If you’re making your own, firstly you need a large body of text. The CogStack team used Wikipedia, and also added in UMLS and MedMentions to their model.</p>

<p>Here’s some code I used to build an open vocab from Wikipedia.</p>

<p>I used gensim to convert a wikipedia dump to plain text, the <code class="highlighter-rouge">clean_wiki()</code> function is basically copied from <a href="https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html">here</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">gensim.corpora</span> <span class="kn">import</span> <span class="n">WikiCorpus</span>


<span class="k">def</span> <span class="nf">clean_wiki</span><span class="p">(</span><span class="n">input_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">output_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
	<span class="n">wiki</span> <span class="o">=</span> <span class="n">WikiCorpus</span><span class="p">(</span><span class="n">input_file</span><span class="p">)</span>
	<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">output_file</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">out</span><span class="p">:</span>
		<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
		<span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">wiki</span><span class="p">.</span><span class="n">get_texts</span><span class="p">():</span>
			<span class="n">out</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="nb">bytes</span><span class="p">(</span><span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">text</span><span class="p">),</span> <span class="s">'utf-8'</span><span class="p">).</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span> <span class="o">+</span> <span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>    
			<span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
			<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
				<span class="k">print</span><span class="p">(</span><span class="s">'Processed '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s">' articles'</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">from</span> <span class="nn">medcat.vocab</span> <span class="kn">import</span> <span class="n">Vocab</span>
<span class="kn">from</span> <span class="nn">medcat.cdb</span> <span class="kn">import</span> <span class="n">CDB</span>
<span class="kn">from</span> <span class="nn">medcat.config</span> <span class="kn">import</span> <span class="n">Config</span>
<span class="kn">from</span> <span class="nn">medcat.cdb_maker</span> <span class="kn">import</span> <span class="n">CDBMaker</span>
<span class="kn">from</span> <span class="nn">medcat.cat</span> <span class="kn">import</span> <span class="n">CAT</span>

<span class="kn">from</span> <span class="nn">medcat.utils.preprocess_wiki</span> <span class="kn">import</span> <span class="n">clean_wiki</span>
<span class="kn">from</span> <span class="nn">medcat.utils.make_vocab</span> <span class="kn">import</span> <span class="n">MakeVocab</span>


<span class="c1"># Build a CDB to and embedding set
</span>
<span class="c1"># Load wikipedia,
# This step can take hours!
</span>
<span class="n">wiki</span> <span class="o">=</span> <span class="s">"../data/enwiki-20230601-pages-articles-multistream.xml.bz2"</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="s">"../data/wikipedia_corpus.txt"</span>

<span class="n">clean_wiki</span><span class="p">(</span><span class="n">wiki</span><span class="p">,</span> <span class="n">corpus</span><span class="p">)</span>

<span class="c1"># Make a fresh vocab
# Generate a set of word embeddings from the corpus
</span>
<span class="k">def</span> <span class="nf">articles</span><span class="p">(</span><span class="nb">file</span><span class="p">):</span>
<span class="c1"># Generator function to read wikipedia article by article, as it is too large to read into memory.
</span>	<span class="k">with</span> <span class="n">Path</span><span class="p">(</span><span class="nb">file</span><span class="p">).</span><span class="nb">open</span><span class="p">()</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
		<span class="n">buf</span> <span class="o">=</span> <span class="s">""</span>
		<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">file</span><span class="p">:</span>
			<span class="k">if</span> <span class="n">line</span> <span class="o">==</span> <span class="s">""</span><span class="p">:</span>
				<span class="n">buf</span> <span class="o">+=</span> <span class="nb">file</span><span class="p">.</span><span class="n">readline</span><span class="p">()</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="k">yield</span> <span class="n">buf</span>
				<span class="n">buf</span> <span class="o">=</span> <span class="s">""</span>

<span class="n">wiki_entries</span> <span class="o">=</span> <span class="n">articles</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">vocab_maker</span> <span class="o">=</span> <span class="n">MakeVocab</span><span class="p">(</span><span class="n">Config</span><span class="p">())</span>

<span class="n">vocab_maker</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="n">wiki_entries</span><span class="p">,</span> <span class="s">"/path/to/wikipedia_vocab"</span><span class="p">)</span>
<span class="n">vocab_maker</span><span class="p">.</span><span class="n">add_vectors</span><span class="p">(</span><span class="s">"/path/to/wikipedia_vocab/data.txt"</span><span class="p">)</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">vocab_maker</span><span class="p">.</span><span class="n">vocab</span>
<span class="n">vocab</span><span class="p">.</span><span class="n">make_unigram_table</span><span class="p">()</span>
<span class="n">vocab</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"/path/to/wikipedia_vocab/vocab.dat"</span><span class="p">)</span>
</code></pre></div></div>

<p>Vocabs are made by <code class="highlighter-rouge">MakeVocab</code> objects. The <code class="highlighter-rouge">.make()</code> method takes an iterator over text, and a path to an output directory for your vocab; it’ll save your corpus in <code class="highlighter-rouge">dir/vocab.txt</code>.</p>

<p><code class="highlighter-rouge">.add_vectors()</code> will generate the word embeddings with Word2Vec, and <code class="highlighter-rouge">.make_unigram_table()</code> will enable negative sampling.</p>

<p>Now we have a list of tons of english words, and their embeddings.</p>

<h3 id="concept-database">Concept Database</h3>
<p>The <a href="https://github.com/CogStack/MedCATtutorials">MedCAT Tutorials</a> cover this well, but in summary, you need to make a CSV file with all your concepts. <a href="https://towardsdatascience.com/medcat-extracting-diseases-from-electronic-health-records-f53c45b3d1c1">Zeljko’s Towards Data Science article</a> covers this well. N.B. it’s worth flagging one of the names for each concept as ‘P’ for primary.</p>

<p>Once you have a Vocab object and CDB object, you can perform the “unsupervised training” of the <em>context embeddings</em> for disambiguation.</p>

<p>You’ll want to do this with some examples of clinical notes. MIMIC-III is a good open datset, though you may well want to train on your own data.</p>

<p>Load the vocab and CDB, and create a CAT object with them. Then train the CAT on your text, this will modify the CDB component by adding/updating the context embeddings but will leave the voacb alone.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">medcat.cat</span> <span class="kn">import</span> <span class="n">CAT</span>
<span class="kn">from</span> <span class="nn">medcat.cdb</span> <span class="kn">import</span> <span class="n">CDB</span>
<span class="kn">from</span> <span class="nn">medcat.vocab</span> <span class="kn">import</span> <span class="n">Vocab</span>

<span class="n">cdb</span> <span class="o">=</span> <span class="n">CDB</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'/cdb_path'</span><span class="p">)</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">Vocab</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'/vocab_path'</span><span class="p">)</span>
<span class="n">cat</span> <span class="o">=</span> <span class="n">CAT</span><span class="p">(</span><span class="n">cdb</span><span class="o">=</span><span class="n">cdb</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">)</span>

<span class="n">cat</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">data_iterator</span><span class="p">)</span>
</code></pre></div></div>

<p>Once you’ve done this you have a complete MedCAT model!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[MedCAT (Kraljevic et al., 2021) is a popular tool for recognising clinical terms in text, and linking to the respective concept from an ontology like SNOMED-CT, ICD-10 or UMLS. It also has the facility to automatically classify concepts based on their context in text.]]></summary></entry><entry><title type="html">This Blog</title><link href="http://localhost:4000/blog/2023/05/23/blog/" rel="alternate" type="text/html" title="This Blog" /><published>2023-05-23T00:00:00+01:00</published><updated>2023-05-23T00:00:00+01:00</updated><id>http://localhost:4000/blog/2023/05/23/blog</id><content type="html" xml:base="http://localhost:4000/blog/2023/05/23/blog/"><![CDATA[<p>I’ve been trying to understand machine learning in more deeply. I need a solid understanding of the innards so I can confidently work on playing with and designing models.</p>

<p>I am following the example (and have copied the site) of Gregory Gundersen. See: <a href="https://gregorygundersen.com/blog/2020/01/12/why-research-blog/">Why I Keep a Research Blog</a> and <a href="https://gregorygundersen.com/blog/2020/06/21/blog-theme/">How I Built This Blog</a>.</p>

<p>TL;DR: Having to explain something is a good way of learning things, and the idea that someone <em>might</em> read it encourages accuracy.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I’ve been trying to understand machine learning in more deeply. I need a solid understanding of the innards so I can confidently work on playing with and designing models.]]></summary></entry></feed>