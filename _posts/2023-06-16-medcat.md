---
title: Understanding MedCAT
subtitle: A simple overview of how MedCAT works internally.
layout: default
date: 2023-06-16
keywords: NLP, Clinial, MedCAT, CogStack
published: true
---

MedCAT {% cite Kraljevic2021-ln %} is a popular tool for recognising clinical terms in text, and linking to the respective concept from an ontology like SNOMED-CT,Â ICD-10 or UMLS. It also has the facility to automatically classify concepts based on their context in text.

MedCAT is available on GitHub: [github.com/CogStack/MedCAT](https://github.com/CogStack/MedCAT), and the MedCAT paper here: [Multi-domain Clinical Natural Language Processing with MedCAT: the Medical Concept Annotation Toolkit](https://arxiv.org/abs/2010.01165).

Official tutorials are [here](https://github.com/CogStack/MedCATtutorials).

Being made of a few quite different parts, MedCAT can be a bit tough to understand well, so here's an explaination of some bits that I found tricky.

## TL;DR
MedCAT passes over input text to find clinical concepts. It does this in a few steps:
1. Spell-checking and correction.
2. Tokenisation and lemmatisation.
3. Entity recognition: it looks for occurances of terms from its database ("Concept Database" -- CDB). When it finds a term, it maps it to the concepts it could be referring to.
4. Linking: If there are multiple concepts the term could be referring to, MedCAT looks at the context of the term to determine which concept it represents.
5. Tagging: If you have added any "metacat models" to your configuration, then those models are used to add tags based on the context of the detected concepts.

## How MedCAT Does It
Before going over the steps in detail, we should establish what a MedCAT model is. It has two main components: a _Vocab_ and a _Concept Database_.

The Vocab is a large set of words and their word embeddings. We'll assume for now that it contains most words in the language. It's essentially a table:

|  WORD  | COUNT | VECTOR |
|--|--|--|
| asprin | 123213 | 0.232141 0.141412 ... |
| hospital | 34244 | 0.254252 0.656568 ... |
| aeroplane | 241415 | 0.54555 0.6366 ... |
| field | 344344 | 0.3555 0.878876 ... |
| teapot | 43452236 | 0.52253 0.23333 ... |

The Concept Database (CDB) contains the set of terms of interest, concept codes, _Context Embeddings_, and mappings between them.

Context embeddings are representations of the words which usually occur around a concept, made by averaging the vectors of the _n_ words either side of the concept. They are used to tell which concept an ambiguous term is referring to.

### The Steps
#### Spelling
MedCAT fixes spelling for terms that appear in its database so that it doesn't miss concepts due to spelling errors. It uses Peter Norvig's spell checker. You don't really need to worry about how this bit works.

#### Tokenisation and Lemmatisation
This uses a model from SpaCy for this bit - this is why you need to download `en_core_web_md-3.4.0` to run MedCAT. Again, this isn't really a part you'll have to worry about.

#### Entity Recgonition
This is where the important bit happens. MedCAT passes over the tokenised text and uses a simple lookup against its list of known concept terms (the CDB), and if there's a match, it marks that term. It can handle some variation in order, and stop words aren't an issue as they were removed in the previous step; more detail in the paper.

Then, there's another pass over all the marked terms. For each term, MedCAT looks it up in the CDB and if it it's mapped to _one_ concept it links to that concept. If there's more than one concept that could match, then is has to do some disambiguation.

##### Disambiguation
This is were the contect embeddings come in handy. MedCAT computes the context embedding for the term as found in the text. It then compares this to context embeddings that it's previously learnt for each concept. It scores the similarity with each candidate concept, filters out any that fall below a threshold, and selects the closest one.

It actually computes this similarity score for all concepts, whether or not disambiguation was needed, and returns is to be used as a confidence metric. 

#### Tagging
MedCAT provides the option to add _metacat_ "meta-models" to the end of the process which use Bi-LSTM models to add metadata to your detected concepts by tagging them.

These models look at the context of the concepts in text. They are configured by the user and need to be trained manually.

You can use these to do things like recognise when a concept is negated, or to find the laterality of a concept.

## Internals and Training
If they don't already, things should start to make more sense as we see how they're built.

By and large, most of the details for building and training a model are here: [MedCAT Tutorials](https://github.com/CogStack/MedCATtutorials).

### Vocab
The vocab is used as a set of correctly spelt words for spell checking, and as a corpus for training word embeddings; MedCAT uses Word2Vec by default.

You can make your own word embeddings any way you like, as long as you put them in a file of the right format you can load them into a vocab -- see the tutorials for this.

If you're making your own, firstly you need a large body of text. The CogStack team used Wikipedia, and also added in UMLS and MedMentions to their model.

Here's some code I used to build an open vocab from Wikipedia.

I used gensim to convert a wikipedia dump to plain text, the `clean_wiki()` function is basically copied from [here](https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html).


```python
from gensim.corpora import WikiCorpus


def clean_wiki(input_file: str, output_file: str):
	wiki = WikiCorpus(input_file)
	with open(output_file, 'w') as out:
		i = 0
		for text in wiki.get_texts():
			out.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\n')    
			i = i + 1
			if (i % 10000 == 0):
				print('Processed ' + str(i) + ' articles')

from pathlib import Path

from medcat.vocab import Vocab
from medcat.cdb import CDB
from medcat.config import Config
from medcat.cdb_maker import CDBMaker
from medcat.cat import CAT

from medcat.utils.preprocess_wiki import clean_wiki
from medcat.utils.make_vocab import MakeVocab


# Build a CDB to and embedding set

# Load wikipedia,
# This step can take hours!

wiki = "../data/enwiki-20230601-pages-articles-multistream.xml.bz2"
corpus = "../data/wikipedia_corpus.txt"

clean_wiki(wiki, corpus)

# Make a fresh vocab
# Generate a set of word embeddings from the corpus

def articles(file):
# Generator function to read wikipedia article by article, as it is too large to read into memory.
	with Path(file).open() as file:
		buf = ""
		for line in file:
			if line == "":
				buf += file.readline() + "\n"
			else:
				yield buf
				buf = ""

wiki_entries = articles(corpus)
vocab_maker = MakeVocab(Config())

vocab_maker.make(wiki_entries, "/path/to/wikipedia_vocab")
vocab_maker.add_vectors("/path/to/wikipedia_vocab/data.txt")
vocab = vocab_maker.vocab
vocab.make_unigram_table()
vocab.save("/path/to/wikipedia_vocab/vocab.dat")
```

Vocabs are made by `MakeVocab` objects. The `.make()` method takes an iterator over text, and a path to an output directory for your vocab; it'll save your corpus in `dir/vocab.txt`.

`.add_vectors()` will generate the word embeddings with Word2Vec, and `.make_unigram_table()` will enable negative sampling.

Now we have a list of tons of english words, and their embeddings.

### Concept Database
The [MedCAT Tutorials](https://github.com/CogStack/MedCATtutorials) cover this well, but in summary, you need to make a CSV file with all your concepts. [Zeljko's Towards Data Science article](https://towardsdatascience.com/medcat-extracting-diseases-from-electronic-health-records-f53c45b3d1c1) covers this well. N.B. it's worth flagging one of the names for each concept as 'P' for primary.

Once you have a Vocab object and CDB object, you can perform the "unsupervised training" of the _context embeddings_ for disambiguation.

You'll want to do this with some examples of clinical notes. MIMIC-III is a good open datset, though you may well want to train on your own data.

Load the vocab and CDB, and create a CAT object with them. Then train the CAT on your text, this will modify the CDB component by adding/updating the context embeddings but will leave the voacb alone.


```python
from medcat.cat import CAT
from medcat.cdb import CDB
from medcat.vocab import Vocab

cdb = CDB.load('/cdb_path')
vocab = Vocab.load('/vocab_path')
cat = CAT(cdb=cdb, vocab=vocab)

cat.train(data_iterator)
```

Once you've done this you have a complete MedCAT model!